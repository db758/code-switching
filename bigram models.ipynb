{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "from collections import Counter\n",
    "import re\n",
    "import math \n",
    "import random \n",
    "import csv \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A little paint job...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Someone called CMU PD on the Muslim Students A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a Chinese student, I support the demonstrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dont About Forget Us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As an alumnus, I support Hong Kong.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Reddit Comments\n",
       "0                              A little paint job...\n",
       "1  Someone called CMU PD on the Muslim Students A...\n",
       "2  As a Chinese student, I support the demonstrat...\n",
       "3                               Dont About Forget Us\n",
       "4                As an alumnus, I support Hong Kong."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv('all_unis_reddit_data.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "for row, col in corpus.iterrows():\n",
    "    if col['Reddit Comments'][-1] not in {'.', '!', '?'}:\n",
    "        data += col['Reddit Comments'] + '. '\n",
    "    else: \n",
    "        data += col['Reddit Comments'] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'all_unis_reddit_data.csv'\n",
    "txt_file = 'all_unis_reddit_data.txt'\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider changing to txt file instead of csv?\n",
    "#need to sentence and word tokenize this \n",
    "#also lowercase? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
    "#string.punctuation = string.punctuation.replace('.', '')\n",
    "#file = open('all_unis_reddit_data.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“”-’‘—'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tis ! is an example , setnce : to see ? if the splitting ; works tis ! is an example , setnce : to see ? if the splitting ; works tis ! is an example , setnce : to see ? if the splitting ; works tis ! is an example , setnce : to see ? if the splitting ; works tis ! is an example , setnce : to see ? if the splitting ; works'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = \"\"\n",
    "test_sentence = 'tis ! is an example, setnce: to see ? if the splitting; \" works\"'\n",
    "for i in range(5):\n",
    "    line_punct_split = re.findall(r\"[\\w']+|[.,!?;''\"\"%&\\'()*+/:;<=>@[\\\\]^_`{|}~“‘—']\", str(test_sentence))\n",
    "    empty += \" \" + \" \".join(line_punct_split)\n",
    "empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tis', '!']\n",
      "['is', 'an', 'example', ',', 'setnce', ':', 'to', 'see', '?']\n",
      "['if', 'the', 'splitting', ';', '``', 'works', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize(test_sentence)\n",
    "for sent in sent_tokenize(test_sentence):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "file_nl_removed = \"\"\n",
    "with open('all_unis_reddit_data.txt') as file:\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
    "        line_punct_split = re.findall(r\"[\\w']+|[.,!?;''\"\"%&\\'()*+/:;<=>@[\\\\]^_`{|}~“‘—']\", str(line_nl_removed))\n",
    "        file_nl_removed += ' ' + ' '.join(line_punct_split)\n",
    "        #break\n",
    "#print(file_nl_removed)\n",
    "    \n",
    "#file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9663775"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_p = open('all_unis_reddit_data_preprocessed.txt', \"w\")\n",
    "file_p.write(file_nl_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_nl_removed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 124615\n",
      "The number of tokens is 2003955\n",
      "The average number of tokens per sentence is 16\n",
      "The number of unique tokens are 48167\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(file_nl_removed)\n",
    "print(\"The number of sentences is\", len(sents)) \n",
    "#prints the number of sentences\n",
    "words = nltk.word_tokenize(file_nl_removed)\n",
    "print(\"The number of tokens is\", len(words)) \n",
    "#prints the number of tokens\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) \n",
    "#prints the average number of tokens per sentence\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
    "#prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423\n"
     ]
    }
   ],
   "source": [
    "#import jieba\n",
    "file = open('baidu_ngrams_to_query.txt', 'r')\n",
    "sents = []\n",
    "for i in range(1423):\n",
    "    sent = file.readline()\n",
    "    sents.append(sent)\n",
    "    \n",
    "#first = file.readline()\n",
    "#sec = file.readline()\n",
    "print(len(sents))\n",
    "#sents.append(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的修理\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sents[435])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized = []\n",
    "for sent in sents:\n",
    "    word_list = jieba.lcut(sent)\n",
    "    word_string = ' '.join(word_list)\n",
    "    word_tokenized.append(word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized = []\n",
    "count = 0\n",
    "for sent in sents:\n",
    "    word_list = jieba.lcut(sent)\n",
    "    if count !=0:\n",
    "        word_string = ' '.join(word_list[1:])\n",
    "        word_tokenized.append(word_string)\n",
    "    else:\n",
    "        word_string = ' '.join(word_list)\n",
    "        word_tokenized.append(word_string)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423\n"
     ]
    }
   ],
   "source": [
    "print(len(word_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is just a test\n"
     ]
    }
   ],
   "source": [
    "words_list = ['this', 'is', 'just', 'a', 'test']\n",
    "test_list = ' '.join(words_list)\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"baidu_ngrams_to_query.txt\", \"w\")\n",
    "text_file.writelines(sent for sent in word_tokenized)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for jieba, since package 'wheel' is not installed.\n",
      "Installing collected packages: jieba\n",
      "    Running setup.py install for jieba ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed jieba-0.42.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.lcut(sents[2489][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "paragraph = u'\\u70ed\\u5e26\\u98ce\\u66b4\\u5c1a\\u5854\\u5c14\\u662f2001\\u5e74\\u5927\\u897f\\u6d0b\\u98d3\\u98ce\\u5b63\\u7684\\u4e00\\u573a\\u57288\\u6708\\u7a7f\\u8d8a\\u4e86\\u52a0\\u52d2\\u6bd4\\u6d77\\u7684\\u5317\\u5927\\u897f\\u6d0b\\u70ed\\u5e26\\u6c14\\u65cb\\u3002\\u5c1a\\u5854\\u5c14\\u4e8e8\\u670814\\u65e5\\u7531\\u70ed\\u5e26\\u5927\\u897f\\u6d0b\\u7684\\u4e00\\u80a1\\u4e1c\\u98ce\\u6ce2\\u53d1\\u5c55\\u800c\\u6210\\uff0c\\u5176\\u5b58\\u5728\\u7684\\u5927\\u90e8\\u5206\\u65f6\\u95f4\\u91cc\\u90fd\\u5728\\u5feb\\u901f\\u5411\\u897f\\u79fb\\u52a8\\uff0c\\u9000\\u5316\\u6210\\u4e1c\\u98ce\\u6ce2\\u540e\\u7a7f\\u8d8a\\u4e86\\u5411\\u98ce\\u7fa4\\u5c9b\\u3002'\n",
    "\n",
    "def zng(paragraph):\n",
    "    sents = []\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"nltk_tokenize_file.py\", line 21, in <module>\n",
      "    import jieba \n",
      "ModuleNotFoundError: No module named 'jieba'\n",
      "sed: stdout: Broken pipe\n",
      "sed: stdout: Broken pipe\n",
      "sed: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!sed 's/\\[url\\]//g;s/(url)//g;' < cleaned_baidu_tieba.txt | sed 's/()//g;s/ )/)/g;' | sed 's/,)/)/g;' | python nltk_tokenize_file.py > cleaned_final_baidu-toks.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common bigrams:  [((',', 'and'), 6499), ((',', 'but'), 6003), (('of', 'the'), 5343), (('in', 'the'), 5155), (('it', \"'s\"), 4724)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    bigram.extend(list(ngrams(sequence, 2)))  \n",
    "\n",
    "def removal(x):     \n",
    "#removes ngrams containing only stopwords\n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)\n",
    "\n",
    "#bigram = removal(bigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here.\n",
    "            \n",
    "ngrams_all = {1:[], 2:[]}\n",
    "for i in range(2):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "ngrams_voc = {1:set([]), 2:set([])}\n",
    "for i in range(2):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "total_ngrams = {1:-1, 2:-1}\n",
    "total_voc = {1:-1, 2:-1}\n",
    "for i in range(2):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[]}\n",
    "for i in range(2):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(2):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('.',), 0.05038630627736683], [('the',), 0.031000968581336837], [(',',), 0.02746990689279303], [('to',), 0.023493239028049683], [('i',), 0.02247568997954129], [('a',), 0.01872133552578101], [('and',), 0.01736884732944791], [('of',), 0.014179090591245945], [('you',), 0.01362330706378828], [('it',), 0.013339784013217365]]\n",
      "\n",
      "Most common bigrams:  [[(',', 'and'), 0.002758987188536875], [(',', 'but'), 0.0025484552430731383], [('of', 'the'), 0.0022683119285447787], [('in', 'the'), 0.002188513529860943], [('it', \"'s\"), 0.0020055714562825745], [(',', 'i'), 0.0019024277813880423], [('if', 'you'), 0.0018183847870295344], [('do', \"n't\"), 0.0016889246189520349], [('i', \"'m\"), 0.0016685505597136086], [(':', '/'), 0.0015216875493699535]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(2):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 hours to run smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and then prediction on english codeswitched sentences ? or just pulling out the prob that the bigram occurs ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kenlm.Model('kenlm/lm/test.arpa')\n",
    "print(model.score(file_nl_removed, bos = True, eos = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = file_nl_removed\n",
    "#print(sentence)\n",
    "print(model.score(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that total full score = direct score\n",
    "def score(s):\n",
    "    return sum(prob for prob, _, _ in model.full_scores(s))\n",
    "\n",
    "#assert (abs(score(sentence) - model.score(sentence)) < 1e-3)\n",
    "\n",
    "# Show scores and n-gram matches\n",
    "words = ['<s>'] + sentence.split() + ['</s>']\n",
    "for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))\n",
    "\n",
    "# Find out-of-vocabulary words\n",
    "for w in words:\n",
    "    if not w in model:\n",
    "        print('\"{0}\" is an OOV'.format(w))\n",
    "\n",
    "#Stateful query\n",
    "state = kenlm.State()\n",
    "state2 = kenlm.State()\n",
    "#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).\n",
    "model.BeginSentenceWrite(state)\n",
    "accum = 0.0\n",
    "accum += model.BaseScore(state, \"a\", state2)\n",
    "accum += model.BaseScore(state2, \"sentence\", state)\n",
    "#score defaults to bos = True and eos = True.  Here we'll check without the end\n",
    "#of sentence marker.  \n",
    "assert (abs(accum - model.score(\"a sentence\", eos = False)) < 1e-3)\n",
    "accum += model.BaseScore(state, \"</s>\", state2)\n",
    "assert (abs(accum - model.score(\"a sentence\")) < 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split text file by sentence \n",
    "pandas explode ?? \n",
    "\n",
    "\n",
    "bin/query text.binary <data > output.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
